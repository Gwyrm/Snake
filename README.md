# ğŸ Snake AI - Apprentissage par Renforcement avec PyTorch

Un projet d'intelligence artificielle pour crÃ©er des agents Snake qui apprennent Ã  jouer grÃ¢ce au **Deep Q-Learning** avec PyTorch et interface graphique PyGame.

## ğŸ¯ FonctionnalitÃ©s

- ğŸ§  **3 agents diffÃ©rents** avec complexitÃ© croissante
- ğŸ® **Interface graphique** en temps rÃ©el avec PyGame
- ğŸ“Š **Visualisation** des performances en temps rÃ©el
- ğŸ”„ **SystÃ¨me universel** pour entraÃ®ner n'importe quel agent
- ğŸ› ï¸ **Utilitaires** de gestion et nettoyage du projet

## ğŸš€ Installation

```bash
# Cloner le projet
git clone <votre-repo>
cd Snake

# CrÃ©er un environnement virtuel
python -m venv snake_env
source snake_env/bin/activate  # Linux/Mac
# ou snake_env\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ¤– Les Agents

### 1. Agent Original (`agent.py`)
- **Architecture**: 11â†’256â†’3 neurones
- **FonctionnalitÃ©s**: DQN basique, Îµ-greedy
- **Performance**: Rapide Ã  entraÃ®ner, scores Ã©levÃ©s mais instables

### 2. Agent AmÃ©liorÃ© (`improved_agent.py`)
- **Architecture**: 16â†’256â†’3 neurones
- **FonctionnalitÃ©s**: DÃ©tection de piÃ¨ges (Flood Fill), rÃ©compenses optimisÃ©es
- **Performance**: Plus stable, Ã©vite mieux l'auto-collision

### 3. Agent Enhanced (`enhanced_training.py`)
- **Architecture**: 16â†’512â†’3 neurones
- **FonctionnalitÃ©s**: Planification long-terme (Î³=0.95), architecture optimisÃ©e
- **Performance**: Meilleur compromis stabilitÃ©/performance

## ğŸ® Utilisation

### Lancement Rapide (Interface Graphique)

```bash
# Mode interactif - choisir l'agent
python launch_gui.py

# Mode direct - spÃ©cifier l'agent
python launch_gui.py Agent          # Agent original
python launch_gui.py ImprovedAgent  # Agent amÃ©liorÃ©
python launch_gui.py EnhancedAgent  # Agent enhanced
```

### SystÃ¨me Universel (AvancÃ©)

```python
from universal_gui_trainer import UniversalGUITrainer

# EntraÃ®ner n'importe quel agent
trainer = UniversalGUITrainer('Agent')
trainer.train_with_gui()

# Test rapide sans GUI
trainer.quick_test(num_games=10)
```

### Jouer avec un ModÃ¨le PrÃ©-entraÃ®nÃ©

```bash
python play.py
```

## ğŸ› ï¸ Utilitaires

```bash
# Voir les agents disponibles
python utils.py list-agents

# Statistiques du projet
python utils.py stats

# Nettoyage
python utils.py clean-cache    # Cache Python
python utils.py clean-models   # ModÃ¨les sauvegardÃ©s
python utils.py clean-all      # Nettoyage complet
```

## ğŸ“ Structure du Projet

```
Snake/
â”œâ”€â”€ ğŸ¤– Agents
â”‚   â”œâ”€â”€ agent.py              # Agent original
â”‚   â”œâ”€â”€ improved_agent.py     # Agent amÃ©liorÃ©
â”‚   â””â”€â”€ enhanced_training.py  # Agent enhanced
â”œâ”€â”€ ğŸ® Interface & Jeu
â”‚   â”œâ”€â”€ snake_game.py         # Moteur de jeu PyGame
â”‚   â”œâ”€â”€ launch_gui.py         # Lancement GUI simple
â”‚   â””â”€â”€ universal_gui_trainer.py # SystÃ¨me universel
â”œâ”€â”€ ğŸ§  ModÃ¨le IA
â”‚   â”œâ”€â”€ model.py              # RÃ©seau neuronal
â”‚   â””â”€â”€ helper.py             # Visualisation
â”œâ”€â”€ ğŸ¯ Utilisation
â”‚   â”œâ”€â”€ play.py               # Jouer avec modÃ¨le
â”‚   â””â”€â”€ utils.py              # Utilitaires projet
â”œâ”€â”€ ğŸ“„ Documentation
â”‚   â”œâ”€â”€ README.md             # Ce fichier
â”‚   â””â”€â”€ IMPROVEMENTS_SUMMARY.md # DÃ©tails techniques
â”œâ”€â”€ ğŸ“¦ Configuration
â”‚   â””â”€â”€ requirements.txt      # DÃ©pendances
â””â”€â”€ ğŸ’¾ ModÃ¨les (gÃ©nÃ©rÃ©)
    â””â”€â”€ model/                # ModÃ¨les sauvegardÃ©s
```

## ğŸ”§ Technologies

- **ğŸ Python 3.8+**
- **ğŸ”¥ PyTorch** - Apprentissage profond
- **ğŸ® PyGame** - Interface graphique
- **ğŸ“Š Matplotlib** - Visualisation
- **ğŸ”¢ NumPy** - Calculs numÃ©riques

## ğŸ¯ Algorithme

Le projet utilise le **Deep Q-Learning (DQN)** :

1. **Ã‰tat** : Position snake, nourriture, dangers (11-16 variables)
2. **Actions** : Tout droit, tourner gauche/droite
3. **RÃ©compenses** : +10 nourriture, -10 collision, bonus optimisÃ©s
4. **RÃ©seau** : Fully connected layers avec ReLU
5. **EntraÃ®nement** : Experience replay + target network

## ğŸ“ˆ RÃ©sultats

| Agent | Score Moyen | Record | StabilitÃ© | FonctionnalitÃ©s |
|-------|-------------|---------|-----------|-----------------|
| Original | 33.7 | 67 | Â±12.3 | DQN basique |
| AmÃ©liorÃ© | 6.9 | 19 | Â±4.1 | + Flood Fill |
| Enhanced | ~40 | 60+ | OptimisÃ© | + Long-term Planning |

## ğŸš€ FonctionnalitÃ©s AvancÃ©es

### DÃ©tection de PiÃ¨ges (Flood Fill)
Algorithme qui analyse l'espace libre autour du snake pour Ã©viter de se coincer.

### SystÃ¨me de RÃ©compenses Intelligent
- RÃ©compenses progressives selon la taille
- Bonus pour se rapprocher de la nourriture
- PÃ©nalitÃ©s douces pour encourager l'exploration

### Interface Universelle
Le systÃ¨me peut automatiquement dÃ©tecter et utiliser n'importe quelle classe d'agent compatible.

## ğŸ® ContrÃ´les pendant l'EntraÃ®nement

- **Fermer la fenÃªtre** : ArrÃªter l'entraÃ®nement
- **Ctrl+C** : ArrÃªt propre dans le terminal
- **Graphiques** : Mise Ã  jour automatique des performances

## ğŸ”„ AmÃ©liorations Futures

- ğŸŒ **RÃ©seau convolutionnel** pour vision spatiale
- ğŸ¯ **A3C/PPO** pour exploration amÃ©liorÃ©e
- ğŸ† **Tournois** entre agents
- ğŸ“± **Interface web** avec Flask/Streamlit
- ğŸ¨ **ThÃ¨mes visuels** personnalisables

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Voir `IMPROVEMENTS_SUMMARY.md` pour les dÃ©tails techniques.

## ğŸ“„ Licence

MIT License - voir LICENSE pour les dÃ©tails.

---

**ğŸ® Amusez-vous bien avec votre Snake AI !** ğŸğŸ¤–