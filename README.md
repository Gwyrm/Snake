# Snake AI - Apprentissage par Renforcement avec PyTorch

Ce projet implÃ©mente un jeu Snake contrÃ´lÃ© par une IA utilisant l'apprentissage par renforcement (Deep Q-Learning) avec PyTorch.

## ğŸ® FonctionnalitÃ©s

- **Jeu Snake complet** avec interface graphique PyGame
- **Agent d'IA** utilisant Deep Q-Network (DQN)
- **Apprentissage par renforcement** avec exploration/exploitation
- **Visualisation en temps rÃ©el** des performances
- **Sauvegarde/chargement** des modÃ¨les entraÃ®nÃ©s

## ğŸ“ Structure du projet

```
â”œâ”€â”€ snake_game.py    # Logique du jeu Snake
â”œâ”€â”€ agent.py         # Agent d'IA et boucle d'entraÃ®nement
â”œâ”€â”€ model.py         # RÃ©seau de neurones DQN
â”œâ”€â”€ helper.py        # Fonctions de visualisation
â”œâ”€â”€ play.py          # Script pour jouer avec un modÃ¨le entraÃ®nÃ©
â”œâ”€â”€ requirements.txt # DÃ©pendances
â””â”€â”€ model/          # Dossier pour sauvegarder les modÃ¨les (crÃ©Ã© automatiquement)
```

## ğŸš€ Installation

1. **Cloner le repository**
```bash
git clone <url_du_repo>
cd Snake
```

2. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

## ğŸ¯ Utilisation

### EntraÃ®ner l'IA

Pour commencer l'entraÃ®nement de l'agent IA :

```bash
python agent.py
```

L'agent va :
- Jouer de nombreuses parties
- Apprendre de ses erreurs
- AmÃ©liorer progressivement ses performances
- Sauvegarder automatiquement le meilleur modÃ¨le

### Jouer avec l'IA entraÃ®nÃ©e

Pour voir l'IA jouer avec un modÃ¨le prÃ©-entraÃ®nÃ© :

```bash
python play.py
```

## ğŸ§  Comment Ã§a fonctionne

### Environnement
- **Espace d'Ã©tat** : 11 variables binaires dÃ©crivant l'environnement
  - Danger immÃ©diat (tout droit, droite, gauche)
  - Direction actuelle (4 directions)
  - Position relative de la nourriture (4 directions)

- **Actions** : 3 actions possibles
  - Continuer tout droit
  - Tourner Ã  droite
  - Tourner Ã  gauche

- **RÃ©compenses** :
  - +10 pour manger de la nourriture
  - -10 pour mourir (collision)
  - 0 pour les autres mouvements

### Architecture du rÃ©seau
- **Couche d'entrÃ©e** : 11 neurones (Ã©tat du jeu)
- **Couche cachÃ©e** : 256 neurones avec activation ReLU
- **Couche de sortie** : 3 neurones (Q-values pour chaque action)

### Algorithme d'apprentissage
- **Deep Q-Learning** avec replay buffer
- **Epsilon-greedy** pour l'exploration
- **Experience replay** pour stabiliser l'apprentissage
- **Target network** implicite via mise Ã  jour des poids

## ğŸ“Š ParamÃ¨tres d'entraÃ®nement

- **Learning rate** : 0.001
- **Gamma (discount factor)** : 0.9
- **Epsilon decay** : 80 - nombre_de_jeux
- **Batch size** : 1000
- **Memory size** : 100,000

## ğŸ”§ Personnalisation

Vous pouvez modifier les paramÃ¨tres dans `agent.py` :

```python
MAX_MEMORY = 100_000  # Taille du replay buffer
BATCH_SIZE = 1000     # Taille des mini-batches
LR = 0.001           # Taux d'apprentissage
```

Ou ajuster l'architecture du rÃ©seau dans `model.py` :

```python
self.model = Linear_QNet(11, 256, 3)  # input, hidden, output
```

## ğŸ“ˆ RÃ©sultats attendus

L'agent devrait :
- Commencer par des scores trÃ¨s bas (0-2)
- Progressivement amÃ©liorer ses performances
- Atteindre des scores de 10+ aprÃ¨s quelques centaines de jeux
- Potentiellement atteindre 20+ avec un entraÃ®nement prolongÃ©

## ğŸ® ContrÃ´les

Pendant l'entraÃ®nement ou le jeu :
- Fermez la fenÃªtre pour arrÃªter le programme
- Les scores et statistiques s'affichent dans la console

## ğŸš¨ DÃ©pannage

**Erreur de police** : Si vous obtenez une erreur avec `arial.ttf`, modifiez dans `snake_game.py` :
```python
font = pygame.font.Font(None, 25)  # Utilise la police par dÃ©faut
```

**Erreur d'affichage** : Si les graphiques ne s'affichent pas, commentez les lignes de visualisation dans `helper.py`.

## ğŸ¯ AmÃ©liorations possibles

- Ajouter des convolutions pour traiter l'image directement
- ImplÃ©menter Double DQN ou Dueling DQN
- Ajouter plus de features Ã  l'espace d'Ã©tat
- Optimiser les hyperparamÃ¨tres
- Ajouter un mode multijoueur

---

ğŸ‰ **Amusez-vous bien avec votre Snake IA !**